{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JAnHncXuUehx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5ThUFFXTk8o"
      },
      "outputs": [],
      "source": [
        "# Railway Track Damage Classification Based on Image Analysis\n",
        "#\n",
        "# Author: Bruno Soares dos Santos\n",
        "# CNN-Based Classification of Railway Track Defects\n",
        "#\n",
        "# This script implements a binary classification model using a CNN with a ResNet50 backbone\n",
        "# for detecting four types of surface defects in railway tracks: Squats, Flakings, Spallings, and Shellings.\n",
        "# Each class is trained using a one-vs-rest approach. The pipeline includes:\n",
        "# - Image preprocessing (resizing, median filter, Canny edge detection)\n",
        "# - Model training with ResNet50\n",
        "# - 5-fold cross-validation\n",
        "# - Evaluation through ROC curves and confusion matrices\n",
        "#\n",
        "# Dependencies: tensorflow, opencv-python, numpy, scikit-learn\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, roc_curve, auc\n",
        "\n",
        "# -----------------------------------\n",
        "# Configuration\n",
        "# -----------------------------------\n",
        "BASE_DIR = '/content/drive/Othercomputers/Laptop I7/doutorado/Dataset-ferrovia/DATASET_ARAIN_4'\n",
        "SAVE_PATH = '/content/drive/Othercomputers/Laptop I7/doutorado/Dados/CNN'\n",
        "CLASSES = ['Squats', 'Flakings', 'Spallings', 'Shellings']\n",
        "IMG_SIZE = (150, 150)\n",
        "\n",
        "# Ensure save directory exists\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "\n",
        "# -----------------------------------\n",
        "# Data Loading and Preprocessing\n",
        "# -----------------------------------\n",
        "def load_images_from_folder(folder, label):\n",
        "    # Load and preprocess images from a specified folder\n",
        "    # Args:\n",
        "    #   folder (str): Path to the image folder\n",
        "    #   label (int): Class label for the images\n",
        "    # Returns:\n",
        "    #   tuple: Lists of preprocessed images and corresponding labels\n",
        "    images, labels = [], []\n",
        "    for filename in os.listdir(folder):\n",
        "        img_path = os.path.join(folder, filename)\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is not None:\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "            img = cv2.resize(img, IMG_SIZE)  # Resize to 150x150\n",
        "            img = cv2.medianBlur(img, 5)  # Apply median filter\n",
        "            img = cv2.Canny(img, 100, 200)  # Apply Canny edge detection\n",
        "            images.append(img)\n",
        "            labels.append(label)\n",
        "    return images, labels\n",
        "\n",
        "def create_binary_dataset(target_class, images, labels):\n",
        "    # Create a balanced binary dataset for a specific class\n",
        "    # Args:\n",
        "    #   target_class (int): Target class index\n",
        "    #   images (np.array): Array of images\n",
        "    #   labels (np.array): Array of labels\n",
        "    # Returns:\n",
        "    #   tuple: Binary dataset images and labels\n",
        "    target_indices = np.where(labels == target_class)[0]\n",
        "    non_target_indices = np.where(labels != target_class)[0]\n",
        "    non_target_sample = np.random.choice(non_target_indices, size=len(target_indices), replace=False)\n",
        "    binary_indices = np.concatenate([target_indices, non_target_sample])\n",
        "    binary_labels = np.concatenate([np.ones(len(target_indices)), np.zeros(len(target_indices))])\n",
        "    return images[binary_indices], binary_labels\n",
        "\n",
        "# -----------------------------------\n",
        "# Model Creation\n",
        "# -----------------------------------\n",
        "def create_model():\n",
        "    # Create a CNN model using ResNet50 backbone\n",
        "    # Returns:\n",
        "    #   tf.keras.Model: Compiled model for binary classification\n",
        "    base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
        "    base_model.trainable = False  # Freeze ResNet50 weights\n",
        "    model = tf.keras.models.Sequential([\n",
        "        base_model,\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(512, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')  # Binary classification\n",
        "    ])\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy', tf.keras.metrics.Recall(name='recall')])\n",
        "    return model\n",
        "\n",
        "# -----------------------------------\n",
        "# Evaluation Metrics\n",
        "# -----------------------------------\n",
        "def calculate_specificity(y_true, y_pred):\n",
        "    # Calculate specificity from true and predicted labels\n",
        "    # Args:\n",
        "    #   y_true (np.array): True labels\n",
        "    #   y_pred (np.array): Predicted labels\n",
        "    # Returns:\n",
        "    #   float: Specificity score\n",
        "    tn, fp, _, _ = confusion_matrix(y_true, y_pred).ravel()\n",
        "    return tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "\n",
        "def save_coords_to_txt(coords, filename):\n",
        "    # Save coordinates to a text file\n",
        "    # Args:\n",
        "    #   coords (list): List of (x, y) coordinates\n",
        "    #   filename (str): Output file path\n",
        "    with open(filename, 'w') as f:\n",
        "        for x, y in coords:\n",
        "            f.write(f\"({x:.4f},{y:.4f})\\n\")\n",
        "\n",
        "# -----------------------------------\n",
        "# Model Training and Evaluation\n",
        "# -----------------------------------\n",
        "def train_and_evaluate_model(data, class_name):\n",
        "    # Train and evaluate the model for a specific class\n",
        "    # Args:\n",
        "    #   data (dict): Dictionary containing train, validation, and test datasets\n",
        "    #   class_name (str): Name of the class being evaluated\n",
        "    # Returns:\n",
        "    #   dict: Training history\n",
        "    X_train, y_train = data['X_train'], data['y_train']\n",
        "    X_val, y_val = data['X_val'], data['y_val']\n",
        "\n",
        "    # Convert grayscale images to RGB and normalize\n",
        "    X_train_rgb = np.stack([cv2.cvtColor(img, cv2.COLOR_GRAY2RGB) for img in X_train]) / 255.0\n",
        "    X_val_rgb = np.stack([cv2.cvtColor(img, cv2.COLOR_GRAY2RGB) for img in X_val]) / 255.0\n",
        "\n",
        "    model = create_model()\n",
        "    history = model.fit(X_train_rgb, y_train, validation_data=(X_val_rgb, y_val),\n",
        "                        epochs=20, verbose=1)\n",
        "\n",
        "    # Calculate specificity for each epoch\n",
        "    specificity_train, specificity_val = [], []\n",
        "    for epoch in range(20):\n",
        "        y_train_pred = (model.predict(X_train_rgb).flatten() >= 0.5).astype(int)\n",
        "        y_val_pred = (model.predict(X_val_rgb).flatten() >= 0.5).astype(int)\n",
        "        tn_train = np.sum((y_train == 0) & (y_train_pred == 0))\n",
        "        fp_train = np.sum((y_train == 0) & (y_train_pred == 1))\n",
        "        tn_val = np.sum((y_val == 0) & (y_val_pred == 0))\n",
        "        fp_val = np.sum((y_val == 0) & (y_val_pred == 1))\n",
        "        specificity_train.append(tn_train / (tn_train + fp_train) if (tn_train + fp_train) > 0 else 0.0)\n",
        "        specificity_val.append(tn_val / (tn_val + fp_val) if (tn_val + fp_val) > 0 else 0.0)\n",
        "\n",
        "    # Save specificity curves\n",
        "    epochs = range(1, 21)\n",
        "    save_coords_to_txt(list(zip(epochs, specificity_train)),\n",
        "                       os.path.join(SAVE_PATH, f'learning_specificity_{class_name}.txt'))\n",
        "    save_coords_to_txt(list(zip(epochs, specificity_val)),\n",
        "                       os.path.join(SAVE_PATH, f'learning_val_specificity_{class_name}.txt'))\n",
        "\n",
        "    # Print learning curves\n",
        "    print(f\"\\nLearning Curves for {class_name}:\")\n",
        "    print(f\"Epochs: {list(epochs)}\")\n",
        "    print(f\"Training Accuracy: {history.history['accuracy']}\")\n",
        "    print(f\"Validation Accuracy: {history.history['val_accuracy']}\")\n",
        "    print(f\"Training Loss: {history.history['loss']}\")\n",
        "    print(f\"Validation Loss: {history.history['val_loss']}\")\n",
        "    print(f\"Training Sensitivity: {history.history['recall']}\")\n",
        "    print(f\"Validation Sensitivity: {history.history['val_recall']}\")\n",
        "    print(f\"Training Specificity: {specificity_train}\")\n",
        "    print(f\"Validation Specificity: {specificity_val}\")\n",
        "\n",
        "    return history\n",
        "\n",
        "# -----------------------------------\n",
        "# Cross-Validation\n",
        "# -----------------------------------\n",
        "def cross_validation_metrics(X, y, n_splits=5):\n",
        "    # Perform k-fold cross-validation and compute metrics\n",
        "    # Args:\n",
        "    #   X (np.array): Input images\n",
        "    #   y (np.array): Labels\n",
        "    #   n_splits (int): Number of folds for cross-validation\n",
        "    # Returns:\n",
        "    #   dict: Mean and standard deviation of accuracy, sensitivity, and specificity\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    accuracies, sensitivities, specificities = [], [], []\n",
        "\n",
        "    for train_index, val_index in kf.split(X):\n",
        "        X_train, X_val = X[train_index], X[val_index]\n",
        "        y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "        # Convert grayscale images to RGB and normalize\n",
        "        X_train_rgb = np.stack([cv2.cvtColor(img, cv2.COLOR_GRAY2RGB) for img in X_train]) / 255.0\n",
        "        X_val_rgb = np.stack([cv2.cvtColor(img, cv2.COLOR_GRAY2RGB) for img in X_val]) / 255.0\n",
        "\n",
        "        model = create_model()\n",
        "        model.fit(X_train_rgb, y_train, epochs=20, verbose=0)\n",
        "\n",
        "        y_pred = (model.predict(X_val_rgb).flatten() >= 0.5).astype(int)\n",
        "        accuracies.append(accuracy_score(y_val, y_pred))\n",
        "        sensitivities.append(recall_score(y_val, y_pred))\n",
        "        specificities.append(calculate_specificity(y_val, y_pred))\n",
        "\n",
        "    return {\n",
        "        'Accuracy': (np.mean(accuracies), np.std(accuracies)),\n",
        "        'Sensitivity': (np.mean(sensitivities), np.std(sensitivities)),\n",
        "        'Specificity': (np.mean(specificities), np.std(specificities))\n",
        "    }\n",
        "\n",
        "# -----------------------------------\n",
        "# ROC Curve and Confusion Matrix\n",
        "# -----------------------------------\n",
        "def generate_and_save_roc_curve(X_data_rgb, y_data, model, class_name, dataset_name):\n",
        "    # Generate and save ROC curve data\n",
        "    # Args:\n",
        "    #   X_data_rgb (np.array): Preprocessed RGB images\n",
        "    #   y_data (np.array): True labels\n",
        "    #   model (tf.keras.Model): Trained model\n",
        "    #   class_name (str): Name of the class\n",
        "    #   dataset_name (str): Dataset type (Test/Validation)\n",
        "    # Returns:\n",
        "    #   float: AUC score\n",
        "    y_pred_prob = model.predict(X_data_rgb).flatten()\n",
        "    fpr, tpr, _ = roc_curve(y_data, y_pred_prob)\n",
        "    auc_score = auc(fpr, tpr)\n",
        "\n",
        "    with open(os.path.join(SAVE_PATH, f'roc_curve_{class_name}_{dataset_name}.txt'), 'w') as f:\n",
        "        for x, y in zip(fpr, tpr):\n",
        "            f.write(f\"({x:.4f},{y:.4f})\\n\")\n",
        "\n",
        "    print(f\"{class_name} ROC AUC for {dataset_name} = {auc_score:.4f}\")\n",
        "    return auc_score\n",
        "\n",
        "def save_confusion_matrix_and_percentages(y_true, y_pred, class_name, dataset_name):\n",
        "    # Save confusion matrix and its percentages\n",
        "    # Args:\n",
        "    #   y_true (np.array): True labels\n",
        "    #   y_pred (np.array): Predicted labels\n",
        "    #   class_name (str): Name of the class\n",
        "    #   dataset_name (str): Dataset type (Test/Validation)\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    TN, FP, FN, TP = conf_matrix.ravel()\n",
        "    total = TN + FP + FN + TP\n",
        "    conf_matrix_percentage = conf_matrix / total * 100 if total > 0 else conf_matrix\n",
        "\n",
        "    with open(os.path.join(SAVE_PATH, f'confusion_matrix_{class_name}_{dataset_name}.txt'), 'w') as f:\n",
        "        f.write(f\"Confusion Matrix (Numerical) for {dataset_name}:\\n\")\n",
        "        f.write(f\"TN: {TN}, FP: {FP}, FN: {FN}, TP: {TP}\\n\\n\")\n",
        "        f.write(f\"Confusion Matrix (Percentage) for {dataset_name}:\\n\")\n",
        "        f.write(f\"TN: {conf_matrix_percentage[0,0]:.2f}%, FP: {conf_matrix_percentage[0,1]:.2f}%\\n\")\n",
        "        f.write(f\"FN: {conf_matrix_percentage[1,0]:.2f}%, TP: {conf_matrix_percentage[1,1]:.2f}%\\n\")\n",
        "\n",
        "    print(f\"{class_name} Confusion Matrix for {dataset_name} saved.\")\n",
        "\n",
        "# -----------------------------------\n",
        "# Main Execution\n",
        "# -----------------------------------\n",
        "def main():\n",
        "    # Main function to execute the pipeline: data loading, model training, cross-validation, and evaluation\n",
        "\n",
        "    # Load and preprocess images\n",
        "    all_images, all_labels = [], []\n",
        "    paths = {cls: os.path.join(BASE_DIR, cls) for cls in CLASSES}\n",
        "    for label, path in enumerate(paths.values()):\n",
        "        images, labels = load_images_from_folder(path, label)\n",
        "        all_images.extend(images)\n",
        "        all_labels.extend(labels)\n",
        "\n",
        "    images = np.array(all_images)\n",
        "    labels = np.array(all_labels)\n",
        "\n",
        "    # Create binary datasets\n",
        "    binary_datasets = {}\n",
        "    for class_idx, class_name in enumerate(CLASSES):\n",
        "        X_bin, y_bin = create_binary_dataset(class_idx, images, labels)\n",
        "        X_train, X_temp, y_train, y_temp = train_test_split(X_bin, y_bin, test_size=0.3,\n",
        "                                                             random_state=42, stratify=y_bin)\n",
        "        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.33,\n",
        "                                                        random_state=42, stratify=y_temp)\n",
        "        binary_datasets[class_name] = {\n",
        "            'X_train': X_train, 'y_train': y_train,\n",
        "            'X_val': X_val, 'y_val': y_val,\n",
        "            'X_test': X_test, 'y_test': y_test\n",
        "        }\n",
        "\n",
        "    # Train and evaluate models\n",
        "    for class_name, data in binary_datasets.items():\n",
        "        print(f\"\\nTraining model for class {class_name}...\")\n",
        "        train_and_evaluate_model(data, class_name)\n",
        "\n",
        "        print(f\"\\nCross-validation for class {class_name}...\")\n",
        "        metrics = cross_validation_metrics(data['X_train'], data['y_train'])\n",
        "        print(f\"Accuracy: Mean = {metrics['Accuracy'][0]:.4f}, Std = {metrics['Accuracy'][1]:.4f}\")\n",
        "        print(f\"Sensitivity: Mean = {metrics['Sensitivity'][0]:.4f}, Std = {metrics['Sensitivity'][1]:.4f}\")\n",
        "        print(f\"Specificity: Mean = {metrics['Specificity'][0]:.4f}, Std = {metrics['Specificity'][1]:.4f}\")\n",
        "\n",
        "        print(f\"\\nProcessing ROC and Confusion Matrix for class {class_name}...\")\n",
        "        X_train_rgb = np.stack([cv2.cvtColor(img, cv2.COLOR_GRAY2RGB) for img in data['X_train']]) / 255.0\n",
        "        X_test_rgb = np.stack([cv2.cvtColor(img, cv2.COLOR_GRAY2RGB) for img in data['X_test']]) / 255.0\n",
        "        X_val_rgb = np.stack([cv2.cvtColor(img, cv2.COLOR_GRAY2RGB) for img in data['X_val']]) / 255.0\n",
        "\n",
        "        model = create_model()\n",
        "        model.fit(X_train_rgb, data['y_train'], epochs=20, verbose=0)\n",
        "\n",
        "        generate_and_save_roc_curve(X_test_rgb, data['y_test'], model, class_name, \"Test\")\n",
        "        generate_and_save_roc_curve(X_val_rgb, data['y_val'], model, class_name, \"Validation\")\n",
        "\n",
        "        y_pred_test = (model.predict(X_test_rgb).flatten() >= 0.5).astype(int)\n",
        "        y_pred_val = (model.predict(X_val_rgb).flatten() >= 0.5).astype(int)\n",
        "\n",
        "        save_confusion_matrix_and_percentages(data['y_test'], y_pred_test, class_name, \"Test\")\n",
        "        save_confusion_matrix_and_percentages(data['y_val'], y_pred_val, class_name, \"Validation\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}